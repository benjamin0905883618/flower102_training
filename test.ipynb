{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811ac3f6-42f3-4961-9a63-14a5330f0c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.datasets import DatasetFolder \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from RealESRGAN import RealESRGAN\n",
    "import os\n",
    "import torchattacks\n",
    "from models_c import *\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "noise_color = 'w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "725a5813-be7a-43ac-a30c-0d2ba1938654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "test_transforms = transforms.Compose([#transforms.RandomCrop((256, 256)),\n",
    "                                       transforms.CenterCrop(224), \n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "toPillow = transforms.ToPILImage()\n",
    "totensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "917d779d-78ec-42c1-aec9-53710295a553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model : flower102\n"
     ]
    }
   ],
   "source": [
    "model = models.densenet201(weights='DenseNet201_Weights.DEFAULT')\n",
    "model = nn.Sequential(*list(model.children())[:-1],ClassifierH2())\n",
    "model = model.to(device)\n",
    "\n",
    "#checkpoint = torch.load(f'./pretrain/flower102_2023052039.pt')\n",
    "checkpoint = torch.load(f'./output_model_{noise_color}/flower102_2023052048_{noise_color}.pt')\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()  \n",
    "print('load model : flower102')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "597dc5e0-f1d0-4fc0-a2ab-10470c3cab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugger :  class -> idx \n",
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14, '15': 15, '16': 16, '17': 17, '18': 18, '19': 19, '20': 20, '21': 21, '22': 22, '23': 23, '24': 24, '25': 25, '26': 26, '27': 27, '28': 28, '29': 29, '30': 30, '31': 31, '32': 32, '33': 33, '34': 34, '35': 35, '36': 36, '37': 37, '38': 38, '39': 39, '40': 40, '41': 41, '42': 42, '43': 43, '44': 44, '45': 45, '46': 46, '47': 47, '48': 48, '49': 49, '50': 50, '51': 51, '52': 52, '53': 53, '54': 54, '55': 55, '56': 56, '57': 57, '58': 58, '59': 59, '60': 60, '61': 61, '62': 62, '63': 63, '64': 64, '65': 65, '66': 66, '67': 67, '68': 68, '69': 69, '70': 70, '71': 71, '72': 72, '73': 73, '74': 74, '75': 75, '76': 76, '77': 77, '78': 78, '79': 79, '80': 80, '81': 81, '82': 82, '83': 83, '84': 84, '85': 85, '86': 86, '87': 87, '88': 88, '89': 89, '90': 90, '91': 91, '92': 92, '93': 93, '94': 94, '95': 95, '96': 96, '97': 97, '98': 98, '99': 99, '100': 100, '101': 101}\n",
      "Debugger :  class -> idx \n",
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14, '15': 15, '16': 16, '17': 17, '18': 18, '19': 19, '20': 20, '21': 21, '22': 22, '23': 23, '24': 24, '25': 25, '26': 26, '27': 27, '28': 28, '29': 29, '30': 30, '31': 31, '32': 32, '33': 33, '34': 34, '35': 35, '36': 36, '37': 37, '38': 38, '39': 39, '40': 40, '41': 41, '42': 42, '43': 43, '44': 44, '45': 45, '46': 46, '47': 47, '48': 48, '49': 49, '50': 50, '51': 51, '52': 52, '53': 53, '54': 54, '55': 55, '56': 56, '57': 57, '58': 58, '59': 59, '60': 60, '61': 61, '62': 62, '63': 63, '64': 64, '65': 65, '66': 66, '67': 67, '68': 68, '69': 69, '70': 70, '71': 71, '72': 72, '73': 73, '74': 74, '75': 75, '76': 76, '77': 77, '78': 78, '79': 79, '80': 80, '81': 81, '82': 82, '83': 83, '84': 84, '85': 85, '86': 86, '87': 87, '88': 88, '89': 89, '90': 90, '91': 91, '92': 92, '93': 93, '94': 94, '95': 95, '96': 96, '97': 97, '98': 98, '99': 99, '100': 100, '101': 101}\n"
     ]
    }
   ],
   "source": [
    "clean_set = DatasetFolder(f'flower102_test_set_{noise_color}', loader=lambda x: Image.open(x), extensions=\"jpg\", transform = test_transforms)\n",
    "adv_set = DatasetFolder(f'flower_test_adv_set_{noise_color}', loader=lambda x: Image.open(x), extensions=\"jpg\", transform = test_transforms)\n",
    "test_set = ConcatDataset([clean_set, adv_set])\n",
    "#test_set = DatasetFolder(f'flower102/test', loader=lambda x: Image.open(x), extensions=\"jpg\", transform = test_transforms)\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e527db-09c9-4e4c-a2f3-1c742bd680d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1202, device='cuda:0')\n",
      "1638\n",
      "Accuracy  = 0.7338217496871948\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "accuracy = 0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "            \n",
    "        total += imgs.shape[0]\n",
    "            \n",
    "        outputs = model(imgs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        #print(preds)\n",
    "        #print(labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy += torch.sum(preds == labels.data)\n",
    "print(accuracy)\n",
    "print(total)\n",
    "print(f'Accuracy  = {accuracy / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1868ae0-243a-43a1-848a-7423646327db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
